{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Notebook\n",
    "\n",
    "This notebook will perform model selection on a Kernel SVM to predict binary classification\n",
    "\n",
    "- Draw Result\n",
    "- Not Draw Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import sem\n",
    "\n",
    "numFolds = 5\n",
    "\n",
    "date = '20170212'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('/Users/mtetkosk/Google Drive/Data Science Projects/data/processed/%s_Matches_w_Features_DrawResults.csv'%(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage_Cat\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        print col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['Stage_Cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280\n",
      "(2280, 212)\n"
     ]
    }
   ],
   "source": [
    "print len(data)\n",
    "print data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "home_goals = data['home_team_goal']\n",
    "away_goals = data['away_team_goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['home_team_goal']\n",
    "del data['away_team_goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 210)\n"
     ]
    }
   ],
   "source": [
    "print data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First step is to set up training and holdout set\n",
    "def Assign_Train_Test(df):\n",
    "    num = random.randint(1,numFolds)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Train_Test'] = data.apply(Assign_Train_Test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test length 458\n",
      "Train length 1822\n"
     ]
    }
   ],
   "source": [
    "## Chose holdout set as approx 10% of data\n",
    "holdout = data[data['Train_Test']==1]\n",
    "train = data[data['Train_Test']!= 1]\n",
    "\n",
    "# Remove the train_test variable from the dataframes\n",
    "del holdout['Train_Test']\n",
    "del train['Train_Test']\n",
    "\n",
    "print 'Test length ' + str(len(holdout))\n",
    "print 'Train length ' + str(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FitPredict(x_train,y_train,x_test,model):\n",
    "    \n",
    "    fit_model = model.fit(x_train,y_train)\n",
    "    preds = fit_model.predict(x_test)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def ComputeErrorMetric(y_true,y_pred):\n",
    "    \n",
    "    #df = pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n",
    "    # draw\n",
    "    #hw_fp = ((df.y_true != 1) & (df.y_pred == 1))\n",
    "    #hw_tp = ((df.y_true == 1) & (df.y_pred == 1))\n",
    "    #hw_fn = ((df.y_true == 1) & (df.y_pred != 1))\n",
    "    #hw_tn = ((df.y_true != 1) & (df.y_pred != 1))\n",
    "\n",
    "    #true_positive = sum(hw_tp)\n",
    "    #false_positive = sum(hw_fp)\n",
    "    #true_negative = sum(hw_tn)\n",
    "    #false_negative = sum(hw_fn)\n",
    "\n",
    "    #combined_error_metric = 10.0/13.0*false_positive/(false_positive+true_negative)+3.0/13.0*false_negative/(false_negative+true_positive)\n",
    "    \n",
    "    #precision = true_positive / (true_positive + false_positive)\n",
    "    #recall = true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    f1score = f1_score(y_true,y_pred, average='binary')\n",
    "    precision = precision_score(y_true,y_pred, average='binary')\n",
    "    recall = recall_score(y_true,y_pred, average = 'binary')\n",
    "    \n",
    "    return round(f1score,2), round(precision,2), round(recall,2)\n",
    "\n",
    "def FindBParams(params_dict):\n",
    "    b_inner_params = []\n",
    "    \n",
    "    best_score = max(params_dict.values())   #changed to max\n",
    "\n",
    "    for key in params_dict.keys():\n",
    "        if params_dict[key] == best_score:\n",
    "            b_inner_params.append(key)\n",
    "    \n",
    "    vals=b_inner_params[0].split('_')\n",
    "        \n",
    "    kernel = vals[0]\n",
    "    gamma = vals[1]\n",
    "        \n",
    "    return kernel,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mtetkosk/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    405\n",
       "2    363\n",
       "4    358\n",
       "5    355\n",
       "3    341\n",
       "Name: Fold, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use Assign_Train_Test to assign cross-validation folds\n",
    "\n",
    "train['Fold'] = train.apply(Assign_Train_Test,axis = 1)\n",
    "\n",
    "train['Fold'].value_counts()   #All folds are approximately equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEM = 0.24 | Precision = 0.21 | Recall = 0.27\n",
      "CEM = 0.28 | Precision = 0.26 | Recall = 0.3\n",
      "CEM = 0.25 | Precision = 0.23 | Recall = 0.27\n",
      "CEM = 0.2 | Precision = 0.19 | Recall = 0.21\n",
      "CEM = 0.22 | Precision = 0.23 | Recall = 0.21\n",
      "CEM = 0.24 | Precision = 0.22 | Recall = 0.27\n",
      "CEM = 0.26 | Precision = 0.25 | Recall = 0.27\n",
      "CEM = 0.26 | Precision = 0.25 | Recall = 0.27\n",
      "CEM = 0.22 | Precision = 0.21 | Recall = 0.23\n",
      "CEM = 0.3 | Precision = 0.31 | Recall = 0.29\n",
      "CEM = 0.26 | Precision = 0.23 | Recall = 0.29\n",
      "CEM = 0.26 | Precision = 0.25 | Recall = 0.27\n",
      "CEM = 0.2 | Precision = 0.21 | Recall = 0.2\n",
      "CEM = 0.23 | Precision = 0.22 | Recall = 0.24\n",
      "CEM = 0.28 | Precision = 0.29 | Recall = 0.27\n",
      "Fold 1 Error Metric: 0.28\n",
      "Precision = 0.29\n",
      "Recall = 0.28\n",
      "Best Params- Kernel = poly Gamma = 3\n",
      "CEM = 0.36 | Precision = 0.3 | Recall = 0.44\n",
      "CEM = 0.31 | Precision = 0.3 | Recall = 0.32\n",
      "CEM = 0.23 | Precision = 0.24 | Recall = 0.23\n",
      "CEM = 0.25 | Precision = 0.23 | Recall = 0.28\n",
      "CEM = 0.37 | Precision = 0.36 | Recall = 0.38\n",
      "CEM = 0.29 | Precision = 0.24 | Recall = 0.37\n",
      "CEM = 0.25 | Precision = 0.27 | Recall = 0.23\n",
      "CEM = 0.22 | Precision = 0.23 | Recall = 0.22\n",
      "CEM = 0.22 | Precision = 0.2 | Recall = 0.25\n",
      "CEM = 0.34 | Precision = 0.33 | Recall = 0.35\n",
      "CEM = 0.31 | Precision = 0.26 | Recall = 0.38\n",
      "CEM = 0.25 | Precision = 0.26 | Recall = 0.23\n",
      "CEM = 0.22 | Precision = 0.22 | Recall = 0.22\n",
      "CEM = 0.25 | Precision = 0.22 | Recall = 0.29\n",
      "CEM = 0.32 | Precision = 0.31 | Recall = 0.33\n",
      "Fold 2 Error Metric: 0.29\n",
      "Precision = 0.24\n",
      "Recall = 0.37\n",
      "Best Params- Kernel = poly Gamma = 2\n",
      "CEM = 0.24 | Precision = 0.21 | Recall = 0.27\n",
      "CEM = 0.26 | Precision = 0.23 | Recall = 0.29\n",
      "CEM = 0.24 | Precision = 0.24 | Recall = 0.24\n",
      "CEM = 0.3 | Precision = 0.33 | Recall = 0.28\n",
      "CEM = 0.22 | Precision = 0.2 | Recall = 0.23\n",
      "CEM = 0.27 | Precision = 0.24 | Recall = 0.3\n",
      "CEM = 0.26 | Precision = 0.23 | Recall = 0.29\n",
      "CEM = 0.28 | Precision = 0.27 | Recall = 0.29\n",
      "CEM = 0.26 | Precision = 0.29 | Recall = 0.24\n",
      "CEM = 0.21 | Precision = 0.2 | Recall = 0.21\n",
      "CEM = 0.27 | Precision = 0.24 | Recall = 0.3\n",
      "CEM = 0.26 | Precision = 0.24 | Recall = 0.29\n",
      "CEM = 0.24 | Precision = 0.25 | Recall = 0.24\n",
      "CEM = 0.23 | Precision = 0.25 | Recall = 0.21\n",
      "CEM = 0.21 | Precision = 0.22 | Recall = 0.21\n",
      "Fold 3 Error Metric: 0.21\n",
      "Precision = 0.2\n",
      "Recall = 0.23\n",
      "Best Params- Kernel = poly Gamma = 3\n",
      "CEM = 0.38 | Precision = 0.34 | Recall = 0.42\n",
      "CEM = 0.29 | Precision = 0.28 | Recall = 0.29\n",
      "CEM = 0.19 | Precision = 0.18 | Recall = 0.19\n",
      "CEM = 0.29 | Precision = 0.26 | Recall = 0.33\n",
      "CEM = 0.3 | Precision = 0.27 | Recall = 0.34\n",
      "CEM = 0.34 | Precision = 0.31 | Recall = 0.38\n",
      "CEM = 0.26 | Precision = 0.27 | Recall = 0.25\n",
      "CEM = 0.16 | Precision = 0.17 | Recall = 0.15\n",
      "CEM = 0.33 | Precision = 0.3 | Recall = 0.38\n",
      "CEM = 0.27 | Precision = 0.26 | Recall = 0.28\n",
      "CEM = 0.31 | Precision = 0.28 | Recall = 0.34\n",
      "CEM = 0.25 | Precision = 0.26 | Recall = 0.25\n",
      "CEM = 0.17 | Precision = 0.18 | Recall = 0.16\n",
      "CEM = 0.31 | Precision = 0.27 | Recall = 0.35\n",
      "CEM = 0.27 | Precision = 0.26 | Recall = 0.28\n",
      "Fold 4 Error Metric: 0.29\n",
      "Precision = 0.25\n",
      "Recall = 0.34\n",
      "Best Params- Kernel = poly Gamma = 2\n",
      "CEM = 0.24 | Precision = 0.23 | Recall = 0.25\n",
      "CEM = 0.28 | Precision = 0.26 | Recall = 0.31\n",
      "CEM = 0.3 | Precision = 0.29 | Recall = 0.31\n",
      "CEM = 0.24 | Precision = 0.22 | Recall = 0.28\n",
      "CEM = 0.32 | Precision = 0.3 | Recall = 0.34\n",
      "CEM = 0.25 | Precision = 0.23 | Recall = 0.28\n",
      "CEM = 0.28 | Precision = 0.27 | Recall = 0.3\n",
      "CEM = 0.3 | Precision = 0.3 | Recall = 0.31\n",
      "CEM = 0.23 | Precision = 0.2 | Recall = 0.27\n",
      "CEM = 0.29 | Precision = 0.29 | Recall = 0.3\n",
      "CEM = 0.24 | Precision = 0.22 | Recall = 0.27\n",
      "CEM = 0.32 | Precision = 0.32 | Recall = 0.32\n",
      "CEM = 0.28 | Precision = 0.26 | Recall = 0.3\n",
      "CEM = 0.24 | Precision = 0.21 | Recall = 0.27\n",
      "CEM = 0.27 | Precision = 0.27 | Recall = 0.28\n",
      "Fold 5 Error Metric: 0.31\n",
      "Precision = 0.29\n",
      "Recall = 0.34\n",
      "Best Params- Kernel = poly Gamma = 2\n",
      "****************************************************\n",
      "Average Error Metric= 0.276\n"
     ]
    }
   ],
   "source": [
    "## Set up cross-validation loop\n",
    "\n",
    "cv_accuracy = []\n",
    "cv_precision = []\n",
    "cv_recall = []\n",
    "\n",
    "kernel_choice = ['rbf','poly','sigmoid']\n",
    "\n",
    "\n",
    "outer_param_scores = {}\n",
    "outer_error_metric = []\n",
    "\n",
    "for fold in range(1,numFolds+1):\n",
    "    \n",
    "    # Outer Cross-Validation\n",
    "    \n",
    "    cv_train = train[train['Fold'] != fold]\n",
    "    cv_test = train[train['Fold'] == fold]\n",
    "    \n",
    "    del cv_train['Fold']\n",
    "    del cv_test['Fold']\n",
    "    \n",
    "    y_train = cv_train['Result_Target']\n",
    "    x_train = cv_train.copy()\n",
    "    del x_train['Result_Target']\n",
    "\n",
    "    \n",
    "    y_test = cv_test['Result_Target']\n",
    "    del cv_test['Result_Target']\n",
    "    x_test = cv_test.copy()\n",
    "    \n",
    "    # Set up inner cross-validation\n",
    "    \n",
    "    inner_train = cv_train.copy()\n",
    "    del cv_train['Result_Target']\n",
    "    \n",
    "    inner_train['Inner_Fold'] = inner_train.apply(Assign_Train_Test, axis = 1)\n",
    "    \n",
    "    best_hyper_params = {}\n",
    "    #se = {}\n",
    "    \n",
    "    # Iterate thru hyperparameter search\n",
    "    for k in kernel_choice:\n",
    "        if k == 'poly':\n",
    "            gamma_list = [2,3,4]\n",
    "        else:\n",
    "            gamma_list = [.01,.1,.2,.5,1]\n",
    "        for g in gamma_list:\n",
    "            \n",
    "            error_metric = []\n",
    "    \n",
    "            for inner_fold in range(1,numFolds+1):\n",
    "            \n",
    "                #print 'Inner Fold: ' + str(inner_fold)\n",
    "        \n",
    "                inner_cv_train = inner_train[inner_train['Inner_Fold']!= inner_fold]\n",
    "                inner_cv_test = inner_train[inner_train['Inner_Fold']== inner_fold]\n",
    "        \n",
    "                del inner_cv_train['Inner_Fold']\n",
    "                del inner_cv_test['Inner_Fold']\n",
    "        \n",
    "                y_inner_train = inner_cv_train['Result_Target']\n",
    "                del inner_cv_train['Result_Target']\n",
    "                x_inner_train = inner_cv_train.copy()\n",
    "    \n",
    "                y_inner_test = inner_cv_test['Result_Target']\n",
    "                del inner_cv_test['Result_Target']\n",
    "                x_inner_test = inner_cv_test.copy()\n",
    "                \n",
    "                if k == 'poly':\n",
    "                    clf = SVC(kernel = k,degree = g, gamma = 3)\n",
    "                else:\n",
    "                    clf = SVC(kernel = k,gamma = g)\n",
    "                \n",
    "                preds = FitPredict(x_inner_train,y_inner_train,x_inner_test,clf)\n",
    "    \n",
    "                cem, precision,recall = ComputeErrorMetric(y_inner_test,preds)  # Calculate combined error metric\n",
    "        \n",
    "                error_metric.append(cem)\n",
    "                if cem > 0:\n",
    "                    print 'CEM = ' + str(cem) + ' | ' + 'Precision = ' + str(precision) + ' | ' + 'Recall = ' + str(recall)\n",
    "            \n",
    "            avg_error_metric = np.mean(error_metric)\n",
    "            #standard_error = sem(error_metric)\n",
    "            param_names = str(k) + '_' + str(g) \n",
    "            best_hyper_params[param_names] = (avg_error_metric)  #register inner-cv average\n",
    "            #se[param_names] = standard_error\n",
    "            \n",
    "    k,g = FindBParams(best_hyper_params)\n",
    "    \n",
    "    if k == 'poly':\n",
    "        clf = SVC(kernel = k,gamma = 3, degree = float(g))\n",
    "    else:\n",
    "        clf = SVC(kernel = k,gamma = float(g))\n",
    "    \n",
    "    preds = FitPredict(x_train,y_train,x_test,clf)\n",
    "    \n",
    "    cem, precision,recall = ComputeErrorMetric(y_test,preds)\n",
    "    \n",
    "    outer_error_metric.append(cem)\n",
    "    \n",
    "    outer_param_names = str(g) + '_' + str(g)\n",
    "    \n",
    "    print 'Fold ' + str(fold) + ' Error Metric: ' + str(round(cem,2))\n",
    "    print 'Precision = ' + str(precision)\n",
    "    print 'Recall = ' + str(recall)\n",
    "    print 'Best Params- ' + 'Kernel = ' + str(k) + ' Gamma = ' + str(g)\n",
    "    \n",
    "    outer_param_scores[outer_param_names] = cem\n",
    "\n",
    "avg_error_metric_outer = np.mean(outer_error_metric)\n",
    "\n",
    "print '****************************************************'\n",
    "print 'Average Error Metric= ' + str(avg_error_metric_outer)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set Error Metric = 0.32\n",
      "Precision = 0.3\n",
      "Recall = 0.34\n"
     ]
    }
   ],
   "source": [
    "## Prepare for test\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "x_train = train.copy()\n",
    "x_test = holdout.copy()\n",
    "\n",
    "y_train = x_train['Result_Target']\n",
    "del x_train['Result_Target']\n",
    "del x_train['Fold']\n",
    "\n",
    "y_test = x_test['Result_Target']\n",
    "del x_test['Result_Target']\n",
    "\n",
    "clf = SVC(kernel = 'poly',gamma = 3, degree = 3)\n",
    "\n",
    "preds = FitPredict(x_train,y_train,x_test,clf)\n",
    "cem, precision, recall = ComputeErrorMetric(y_test,preds)\n",
    "\n",
    "print 'Holdout Set Error Metric = ' + str(round(cem,2))\n",
    "print 'Precision = ' + str(precision)\n",
    "print 'Recall = ' + str(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>237</td>\n",
       "      <td>99</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>80</td>\n",
       "      <td>42</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>317</td>\n",
       "      <td>141</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  0.0  1.0  All\n",
       "Actual                  \n",
       "0.0        237   99  336\n",
       "1.0         80   42  122\n",
       "All        317  141  458"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion = pd.crosstab(y_test,preds, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_home_odds = x_test['Average_Home_Odds'].copy()\n",
    "test_away_odds = x_test['Average_Away_Odds'].copy()\n",
    "draw_odds = x_test['Average_Draw_Odds'].copy()\n",
    "bet_preds = preds.copy()\n",
    "actual_results = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Away_Odds</th>\n",
       "      <th>Draw_Odds</th>\n",
       "      <th>Home_Odds</th>\n",
       "      <th>Model_Preds</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.64</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.93</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.84</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.10</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.98</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.83</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.35</td>\n",
       "      <td>3.19</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.27</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.50</td>\n",
       "      <td>5.97</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.16</td>\n",
       "      <td>4.54</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.53</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.75</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.07</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.24</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.13</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.74</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.33</td>\n",
       "      <td>4.41</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.36</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.39</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.29</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.19</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.33</td>\n",
       "      <td>5.92</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.73</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.04</td>\n",
       "      <td>4.72</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.68</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.84</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.33</td>\n",
       "      <td>4.73</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.67</td>\n",
       "      <td>7.58</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.93</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.37</td>\n",
       "      <td>4.11</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>4.90</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>2.14</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>1.61</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>1.72</td>\n",
       "      <td>3.68</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2.04</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>8.30</td>\n",
       "      <td>4.37</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>3.92</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>5.37</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>9.13</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>7.38</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>4.28</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>3.41</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>3.15</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>3.96</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>8.84</td>\n",
       "      <td>5.08</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>4.33</td>\n",
       "      <td>3.46</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>3.87</td>\n",
       "      <td>3.42</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>6.14</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>14.67</td>\n",
       "      <td>6.38</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>5.33</td>\n",
       "      <td>3.73</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>3.05</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>2.30</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>3.75</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>2.42</td>\n",
       "      <td>3.39</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>4.42</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>2.23</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2.52</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1.74</td>\n",
       "      <td>3.72</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>2.10</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Away_Odds  Draw_Odds  Home_Odds  Model_Preds  Result\n",
       "0         5.64       3.80       1.61          1.0     0.0\n",
       "1         2.93       3.24       2.30          1.0     0.0\n",
       "2         2.84       3.22       2.37          1.0     0.0\n",
       "3         3.10       3.24       2.22          0.0     0.0\n",
       "4         2.98       3.22       2.35          0.0     0.0\n",
       "5        15.83       6.50       1.17          0.0     0.0\n",
       "6         3.35       3.19       2.19          0.0     1.0\n",
       "7         3.27       3.23       2.23          1.0     0.0\n",
       "8        14.50       5.97       1.20          1.0     0.0\n",
       "9         9.16       4.54       1.36          0.0     0.0\n",
       "10        3.53       3.28       2.07          0.0     0.0\n",
       "11        8.75       4.45       1.39          0.0     0.0\n",
       "12        2.07       3.31       3.51          0.0     1.0\n",
       "13        2.24       3.24       3.15          1.0     0.0\n",
       "14        2.13       3.30       3.38          1.0     0.0\n",
       "15        2.74       3.21       2.52          0.0     0.0\n",
       "16        8.33       4.41       1.39          0.0     1.0\n",
       "17        2.36       3.24       2.98          0.0     0.0\n",
       "18        2.39       3.22       2.90          0.0     0.0\n",
       "19        2.29       3.22       3.18          0.0     1.0\n",
       "20        7.19       4.00       1.44          0.0     1.0\n",
       "21       13.33       5.92       1.20          1.0     0.0\n",
       "22        3.73       3.31       2.02          0.0     1.0\n",
       "23        9.04       4.72       1.33          0.0     0.0\n",
       "24        4.68       3.49       1.76          0.0     0.0\n",
       "25        4.84       3.59       1.74          0.0     0.0\n",
       "26        9.33       4.73       1.33          0.0     1.0\n",
       "27       19.67       7.58       1.13          1.0     0.0\n",
       "28        5.93       3.71       1.59          1.0     0.0\n",
       "29        7.37       4.11       1.45          1.0     0.0\n",
       "..         ...        ...        ...          ...     ...\n",
       "428       2.10       3.28       3.76          0.0     1.0\n",
       "429       4.90       3.75       1.75          0.0     1.0\n",
       "430       2.14       3.39       3.47          0.0     0.0\n",
       "431       1.61       4.00       5.57          0.0     0.0\n",
       "432       1.72       3.68       5.04          0.0     0.0\n",
       "433       2.04       3.40       3.80          0.0     0.0\n",
       "434       8.30       4.37       1.43          0.0     1.0\n",
       "435       3.92       3.35       2.02          0.0     0.0\n",
       "436       5.37       3.52       1.73          0.0     0.0\n",
       "437       9.13       5.00       1.35          0.0     0.0\n",
       "438       7.38       4.40       1.45          0.0     1.0\n",
       "439       4.28       3.60       1.85          1.0     0.0\n",
       "440       3.41       3.20       2.24          1.0     0.0\n",
       "441       3.15       3.20       2.38          0.0     0.0\n",
       "442       3.96       3.43       1.97          0.0     0.0\n",
       "443       8.84       5.08       1.35          0.0     0.0\n",
       "444       4.33       3.46       1.90          0.0     0.0\n",
       "445       3.87       3.42       2.01          0.0     0.0\n",
       "446       6.14       4.00       1.57          1.0     0.0\n",
       "447      14.67       6.38       1.21          0.0     0.0\n",
       "448       5.33       3.73       1.68          1.0     0.0\n",
       "449       3.05       3.32       2.41          0.0     1.0\n",
       "450       2.30       3.30       3.27          0.0     0.0\n",
       "451       3.75       3.22       2.12          0.0     1.0\n",
       "452       2.42       3.39       2.96          0.0     0.0\n",
       "453       4.42       3.60       1.83          0.0     0.0\n",
       "454       2.23       3.23       3.42          0.0     0.0\n",
       "455       2.52       3.32       2.84          0.0     1.0\n",
       "456       1.74       3.72       5.02          0.0     0.0\n",
       "457       2.10       3.42       3.50          1.0     0.0\n",
       "\n",
       "[458 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Betting_df = pd.DataFrame({'Home_Odds':test_home_odds,'Away_Odds':test_away_odds,'Draw_Odds': draw_odds,'Model_Preds':bet_preds,'Result': y_test.copy()})\n",
    "Betting_df = Betting_df.reset_index(drop=True)\n",
    "Betting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Betting Analysis Function\n",
    "\n",
    "def BettingAnalysis(df,purse,bet):\n",
    "    initial_purse = purse\n",
    "    purse_track = []\n",
    "    for match in range(len(df)):\n",
    "        pred = df['Model_Preds'][match]\n",
    "        result = df['Result'][match]\n",
    "        home_odds = df['Home_Odds'][match]\n",
    "        away_odds = df['Away_Odds'][match]\n",
    "        draw_odds = df['Draw_Odds'][match]\n",
    "        \n",
    "        if pred == result:\n",
    "            #if pred == 1:\n",
    "            #    win = round(bet*home_odds,2)-bet\n",
    "            #    purse += win\n",
    "            #if pred == -1: #simulate no bet\n",
    "            #    win = round(bet*away_odds,2)-bet\n",
    "            #    purse += win\n",
    "            #purse_track.append(purse)\n",
    "            if pred == 1:\n",
    "                win = round(bet*draw_odds,2)-bet\n",
    "                purse += win            \n",
    "        else:\n",
    "            purse = purse - bet\n",
    "            purse_track.append(purse)\n",
    "    \n",
    "    if purse > initial_purse:\n",
    "        profit = purse-initial_purse\n",
    "        #return 'You profited ' +str(round(profit,2)) +'!'\n",
    "        return purse_track\n",
    "    if purse == initial_purse:\n",
    "        #return 'You broke even!'\n",
    "        return purse_track\n",
    "    if purse < initial_purse:\n",
    "        loss = purse-initial_purse\n",
    "        #return 'You lost ' + str(abs(round(loss,2))) + 'now you\\'re broke!'\n",
    "        return purse_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "purse_track = BettingAnalysis(Betting_df,100,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10442a8d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGHCAYAAABF4dM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZmVjQXM2ZmZla0vwA/OHpGQeu4qgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1149d6d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(purse_track)\n",
    "plt.xlabel('Match Number')\n",
    "plt.ylabel('Betting Balance $')\n",
    "plt.title('Betting Algorithm Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Betting_df['purse'] = purse_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Betting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
